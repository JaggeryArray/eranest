{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all variable descriptions\n",
    "\n",
    "describe_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for variables related to humidity\n",
    "\n",
    "search_variables(pattern=\"humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: india.json\n",
      "Using GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\temp_india_yearly_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: temp_india_yearly\n",
      "Variables: ['2m_temperature']\n",
      "Date Range: 1941-01-01 to 2024-12-31\n",
      "Frequency: yearly\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\temp_india_yearly_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\temp_india_yearly_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "\u001b[0;32m✓ Successfully loaded GeoJSON file with utf-8 encoding\u001b[0m\n",
      "\u001b[0;32m✓ GeoJSON contains 1 feature(s)\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 1008\n",
      "Max months per chunk: 100\n",
      "Needs chunking: True\n",
      "Will process 11 chunks C1\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1941-01-01 to 1949-04-30\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:29:17,828 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:29:18,491 INFO Request ID is 4cd9bfb3-d026-4cb4-8c56-56f8c62ed59c\n",
      "2025-07-03 14:29:18,858 INFO status has been updated to accepted\n",
      "2025-07-03 14:29:27,866 INFO status has been updated to running\n",
      "2025-07-03 14:29:40,917 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk1\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.47 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.85 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.32 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 28.94 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1949-05-01 to 1957-08-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:29:56,691 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:29:57,371 INFO Request ID is d6b5eda3-f076-4132-83f7-0e983b2f7054\n",
      "2025-07-03 14:29:57,577 INFO status has been updated to accepted\n",
      "2025-07-03 14:30:13,259 INFO status has been updated to running\n",
      "2025-07-03 14:30:21,250 INFO status has been updated to accepted\n",
      "2025-07-03 14:30:32,848 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk2\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.49 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.79 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.29 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 40.75 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 3/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1957-09-01 to 1965-12-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:30:47,434 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:30:48,115 INFO Request ID is e11d5ac5-f695-453d-9e87-9fa5024f45a2\n",
      "2025-07-03 14:30:48,309 INFO status has been updated to accepted\n",
      "2025-07-03 14:31:02,574 INFO status has been updated to running\n",
      "2025-07-03 14:31:10,395 INFO status has been updated to accepted\n",
      "2025-07-03 14:31:22,229 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk3.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk3\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.46 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.78 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.24 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 39.49 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 4/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1966-01-01 to 1974-04-30\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:31:36,956 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:31:37,583 INFO Request ID is a48373b6-51b5-4d57-bd9d-dc7dc928bff1\n",
      "2025-07-03 14:31:37,776 INFO status has been updated to accepted\n",
      "2025-07-03 14:31:52,140 INFO status has been updated to running\n",
      "2025-07-03 14:31:59,943 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk4.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk4.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk4\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.46 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.78 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.24 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 27.63 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 5/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1974-05-01 to 1982-08-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:32:14,573 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:32:15,283 INFO Request ID is ad79b836-d4f1-4e83-b14f-70dcc9e20dd9\n",
      "2025-07-03 14:32:15,882 INFO status has been updated to accepted\n",
      "2025-07-03 14:32:25,122 INFO status has been updated to running\n",
      "2025-07-03 14:32:49,990 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk5.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk5.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk5\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.44 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.84 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.28 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 40.39 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 6/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1982-09-01 to 1990-12-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:33:04,984 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:33:05,650 INFO Request ID is a9cf2713-fe19-4285-976c-05060cf273d6\n",
      "2025-07-03 14:33:05,935 INFO status has been updated to accepted\n",
      "2025-07-03 14:33:20,457 INFO status has been updated to running\n",
      "2025-07-03 14:33:39,993 INFO status has been updated to successful\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;31m✗ Download attempt 1 failed: HTTPSConnectionPool(host='object-store.os-api.cci2.ecmwf.int', port=443): Max retries exceeded with url: /cci2-prod-cache-2/2025-07-03/60b56aa170de37927ec39e414ea079b2.zip (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\u001b[0m\n",
      "  \u001b[0;33m→ Connection error detected. Retrying in 30 seconds...\u001b[0m\n",
      "  → Downloading ERA5 data (attempt 2/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:34:12,000 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:34:12,636 INFO Request ID is 9d2bfe9b-8b85-4b14-810a-8ba309f46de2\n",
      "2025-07-03 14:34:12,828 INFO status has been updated to accepted\n",
      "2025-07-03 14:34:27,303 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk6.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk6.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk6\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.69 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.98 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.68 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 87.56 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 7/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1991-01-01 to 1999-04-30\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:34:42,577 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:34:43,193 INFO Request ID is a896a82e-db4c-4c26-871d-aa95b023b50e\n",
      "2025-07-03 14:34:43,422 INFO status has been updated to accepted\n",
      "2025-07-03 14:34:52,415 INFO status has been updated to running\n",
      "2025-07-03 14:35:34,364 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk7.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk7.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk7\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.57 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.92 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.50 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 56.86 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 8/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 1999-05-01 to 2007-08-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:35:49,495 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:35:50,111 INFO Request ID is d91bac86-1f5e-4f48-9abc-aee79f2dbab7\n",
      "2025-07-03 14:35:50,627 INFO status has been updated to accepted\n",
      "2025-07-03 14:35:59,573 INFO status has been updated to running\n",
      "2025-07-03 14:36:24,252 INFO status has been updated to successful\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;31m✗ Download attempt 1 failed: HTTPSConnectionPool(host='object-store.os-api.cci2.ecmwf.int', port=443): Max retries exceeded with url: /cci2-prod-cache-3/2025-07-03/499028cf649054cff974fd7a1044072d.zip (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\u001b[0m\n",
      "  \u001b[0;33m→ Connection error detected. Retrying in 30 seconds...\u001b[0m\n",
      "  → Downloading ERA5 data (attempt 2/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:36:56,025 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:36:56,769 INFO Request ID is bb6cf7c1-1c59-4d9d-a5bc-ef74f4e249ee\n",
      "2025-07-03 14:36:56,963 INFO status has been updated to accepted\n",
      "2025-07-03 14:37:11,308 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk8.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk8.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk8\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.51 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.88 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.39 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 87.15 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 9/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 2007-09-01 to 2015-12-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Atharva Jagtap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datapi\\api_client.py:83: UserWarning: HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/catalogue/v1/messages (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\n",
      "  warnings.warn(str(exc), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;31m✗ Download attempt 1 failed: HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/processes/reanalysis-era5-single-levels-monthly-means (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\u001b[0m\n",
      "  \u001b[0;33m→ Connection error detected. Retrying in 30 seconds...\u001b[0m\n",
      "  → Downloading ERA5 data (attempt 2/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:37:57,007 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:37:57,888 INFO Request ID is 0a448d03-c3f2-4f12-b5d3-ce79cf97a3af\n",
      "2025-07-03 14:37:58,088 INFO status has been updated to accepted\n",
      "2025-07-03 14:38:31,972 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk9.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk9.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk9\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.50 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.95 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.45 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 70.36 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 10/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 2016-01-01 to 2024-04-30\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:38:46,910 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:38:47,580 INFO Request ID is 48dde4ff-c6d9-41e8-831e-2d78e3c44159\n",
      "2025-07-03 14:38:47,952 INFO status has been updated to accepted\n",
      "2025-07-03 14:38:56,872 INFO status has been updated to running\n",
      "2025-07-03 14:39:38,972 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk10.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk10.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk10\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 108, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.50 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 1402596 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 1402596 to 480168 rows\n",
      "✓ Dataset filtering completed in 0.91 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.41 seconds\n",
      "Final DataFrame shape: (480168, 6)\n",
      "Rows in final dataset: 480168\n",
      "  \u001b[0;32m✓ Filtered data shape: (480168, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 57.10 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 11/11\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-05-01 to 2024-12-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Atharva Jagtap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datapi\\api_client.py:83: UserWarning: HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/catalogue/v1/messages (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\n",
      "  warnings.warn(str(exc), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;31m✗ Download attempt 1 failed: HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/processes/reanalysis-era5-single-levels-monthly-means (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1000)')))\u001b[0m\n",
      "  \u001b[0;33m→ Connection error detected. Retrying in 30 seconds...\u001b[0m\n",
      "  → Downloading ERA5 data (attempt 2/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 14:40:24,448 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-03 14:40:25,067 INFO Request ID is 658a4252-e20d-44e5-ba92-d7ca6dbd8de0\n",
      "2025-07-03 14:40:25,275 INFO status has been updated to accepted\n",
      "2025-07-03 14:40:47,398 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:265: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:282: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: temp_india_yearly_chunk11.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: temp_india_yearly_chunk11.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\for temp change in india\\temp_india_yearly_chunk11\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 1 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/1: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 8, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 8, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.48 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 103896 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 103896 to 35568 rows\n",
      "✓ Dataset filtering completed in 0.06 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.55 seconds\n",
      "Final DataFrame shape: (35568, 6)\n",
      "Rows in final dataset: 35568\n",
      "  \u001b[0;32m✓ Filtered data shape: (35568, 6)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 57.32 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (4837248, 6)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (yearly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to yearly frequency...\n",
      "Sum columns: []\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.97 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (84, 2)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 4446\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: temp_india_yearly_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: temp_india_yearly_output\\temp_india_yearly_yearly_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: temp_india_yearly_output\\temp_india_yearly_unique_latlongs.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\processing\\data_aggregator.py:191: FutureWarning: 'AS' is deprecated and will be removed in a future version, please use 'YS' instead.\n",
      "  result[col] = spatial_agg[col].resample(freq_map[frequency]).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m✓ Raw data exported to: temp_india_yearly_output\\temp_india_yearly_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 709.48 seconds\n",
      "Final dataset shape: (84, 2)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "          t2m  year\n",
      "0  296.386200  1941\n",
      "1  296.234344  1942\n",
      "2  295.743408  1943\n",
      "3  296.361176  1944\n",
      "4  296.111603  1945\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\temp_india_yearly_temp_geojson.json\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(request_id=\"temp_india_yearly\", \n",
    "                             variables=[\"2m_temperature\"],\n",
    "                             start_date=\"1941-1-1\",\n",
    "                             end_date=\"2024-12-31\",\n",
    "                             json_file=\"india.json\",\n",
    "                             frequency=\"yearly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mamba-climate]",
   "language": "python",
   "name": "conda-env-mamba-climate-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
