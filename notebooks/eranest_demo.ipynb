{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import eranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_geojson(request_id, variables, start_date, end_date, json_file, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : datetime, start date of the data\n",
    "# end_date : datetime, end date of the data\n",
    "# json_file : str, path to the geojson file containing the area of interest\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/india.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test\n",
      "Variables: ['2m_temperature', 'total_precipitation', 'surface_pressure', '2m_dewpoint_temperature']\n",
      "Date Range: 2023-01-01 to 2023-01-31\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: ../data/india.json\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: ../data/india.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 31\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 3 chunks C2\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/3\n",
      "==================================================\n",
      "Processing: 2023-01-01 to 2023-01-14\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk1\n",
      "Date range: 2023-01-01 to 2023-01-14\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:33:50,800 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:33:51,721 INFO Request ID is 4fb482aa-ef8a-4cc3-8f46-76633743138f\n",
      "2025-05-29 22:33:51,926 INFO status has been updated to accepted\n",
      "2025-05-29 22:34:13,945 INFO status has been updated to running\n",
      "2025-05-29 22:34:42,921 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk1.zip\n",
      "  ✓ Download completed: test_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.51 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.96 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.52 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 99.66 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/3\n",
      "==================================================\n",
      "Processing: 2023-01-15 to 2023-01-28\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk2\n",
      "Date range: 2023-01-15 to 2023-01-28\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:35:35,249 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:35:36,172 INFO Request ID is 4ea21775-fdc0-4372-a73f-4535dd60f736\n",
      "2025-05-29 22:35:36,377 INFO status has been updated to accepted\n",
      "2025-05-29 22:36:53,176 INFO status has been updated to running\n",
      "2025-05-29 22:37:31,810 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk2.zip\n",
      "  ✓ Download completed: test_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.55 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.86 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.41 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 339.29 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 3/3\n",
      "==================================================\n",
      "Processing: 2023-01-29 to 2023-01-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk3\n",
      "Date range: 2023-01-29 to 2023-01-31\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:41:20,650 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:41:22,085 INFO Request ID is c7d10e2f-8107-4ea6-b2ea-19fb673faac6\n",
      "2025-05-29 22:41:22,286 INFO status has been updated to accepted\n",
      "2025-05-29 22:42:13,181 INFO status has been updated to running\n",
      "2025-05-29 22:42:38,993 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk3.zip\n",
      "  ✓ Download completed: test_chunk3.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk3\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk3\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.51 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 935064 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 935064 to 320112 rows\n",
      "✓ Dataset filtering completed in 0.59 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.10 seconds\n",
      "Final DataFrame shape: (320112, 9)\n",
      "Rows in final dataset: 320112\n",
      "  ✓ Filtered data shape: (320112, 9)\n",
      "  ✓ Chunk completed in 129.30 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (3307824, 9)\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m', 'sp', 'd2m']\n",
      "✓ Aggregation completed in 0.68 seconds\n",
      "✓ Aggregated data shape: (31, 8)\n",
      "✓ Unique lat/long combinations: 4446\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test_output\n",
      "✓ Aggregated data exported to: test_output\\test_daily_data.csv\n",
      "✓ Unique lat/longs exported to: test_output\\test_unique_latlongs.csv\n",
      "✓ Raw data exported to: test_output\\test_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 595.73 seconds\n",
      "Final dataset shape: (31, 8)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m            sp         d2m  year  month  day  \\\n",
      "0  0.000155  289.156586  95582.187500  283.245087  2023      1    1   \n",
      "1  0.000184  288.782196  95621.062500  283.129059  2023      1    2   \n",
      "2  0.000141  288.308716  95639.898438  282.865387  2023      1    3   \n",
      "3  0.000200  287.533905  95647.882812  281.997925  2023      1    4   \n",
      "4  0.000143  287.359467  95756.250000  281.820160  2023      1    5   \n",
      "\n",
      "         date  \n",
      "0  2023-01-01  \n",
      "1  2023-01-02  \n",
      "2  2023-01-03  \n",
      "3  2023-01-04  \n",
      "4  2023-01-05  \n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test\",\n",
    "    variables=[\n",
    "        \"2m_temperature\",\n",
    "        \"total_precipitation\",\n",
    "        \"surface_pressure\",\n",
    "        \"2m_dewpoint_temperature\",\n",
    "    ],\n",
    "    start_date=dt.datetime(2023, 1, 1),\n",
    "    end_date=dt.datetime(2023, 1, 31),\n",
    "    json_file=\"../data/india.json\",\n",
    "    frequency=\"daily\",\n",
    "    resolution=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/latvia.geojson\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test2\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-12-31\n",
      "Frequency: monthly\n",
      "Resolution: 0.1°\n",
      "GeoJSON File: ../data/latvia.geojson\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: ../data/latvia.geojson\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 58.0856°\n",
      "  South: 55.6776°\n",
      "  East:  28.2431°\n",
      "  West:  20.9537°\n",
      "  Area:  7.2894° × 2.4080°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 12\n",
      "Max months per chunk: 10\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C1\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/2\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-10-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk1\n",
      "Date range: 2024-01-01 to 2024-10-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:50:30,167 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:50:30,853 INFO Request ID is 9004e213-bd94-4f44-b480-1126f2b0fece\n",
      "2025-05-29 22:50:31,057 INFO status has been updated to accepted\n",
      "2025-05-29 22:50:45,702 INFO status has been updated to running\n",
      "2025-05-29 22:50:53,791 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1495: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk1.zip\n",
      "  ✓ Download completed: test2_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test2_chunk1\\data_0.nc\n",
      "  - d:\\eranest\\notebooks\\test2_chunk1\\data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 20, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.21 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 36500 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 36500 to 19240 rows\n",
      "✓ Dataset filtering completed in 0.02 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.24 seconds\n",
      "Final DataFrame shape: (19240, 7)\n",
      "Rows in final dataset: 19240\n",
      "  ✓ Filtered data shape: (19240, 7)\n",
      "  ✓ Chunk completed in 29.15 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/2\n",
      "==================================================\n",
      "Processing: 2024-11-01 to 2024-12-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk2\n",
      "Date range: 2024-11-01 to 2024-12-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:51:04,441 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:51:05,071 INFO Request ID is 1975f8ac-b71d-4009-b5b3-45c93fa415c7\n",
      "2025-05-29 22:51:05,262 INFO status has been updated to accepted\n",
      "2025-05-29 22:51:27,482 INFO status has been updated to running\n",
      "2025-05-29 22:51:39,053 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1495: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk2.zip\n",
      "  ✓ Download completed: test2_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test2_chunk2\\data_0.nc\n",
      "  - d:\\eranest\\notebooks\\test2_chunk2\\data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 4, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.21 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 7300 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 7300 to 3848 rows\n",
      "✓ Dataset filtering completed in 0.01 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.23 seconds\n",
      "Final DataFrame shape: (3848, 7)\n",
      "Rows in final dataset: 3848\n",
      "  ✓ Filtered data shape: (3848, 7)\n",
      "  ✓ Chunk completed in 40.99 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (23088, 7)\n",
      "\n",
      "--- Temporal Aggregation (monthly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to monthly frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "✓ Aggregation completed in 0.01 seconds\n",
      "✓ Aggregated data shape: (12, 4)\n",
      "✓ Unique lat/long combinations: 962\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test2_output\n",
      "✓ Aggregated data exported to: test2_output\\test2_monthly_data.csv\n",
      "✓ Unique lat/longs exported to: test2_output\\test2_unique_latlongs.csv\n",
      "✓ Raw data exported to: test2_output\\test2_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 75.25 seconds\n",
      "Final dataset shape: (12, 4)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month\n",
      "0  0.001762  267.574188  2024      1\n",
      "1  0.002134  273.594452  2024      2\n",
      "2  0.000817  276.579803  2024      3\n",
      "3  0.002484  280.693359  2024      4\n",
      "4  0.000811  287.699432  2024      5\n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test2\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=dt.datetime(2024, 1, 1),\n",
    "    end_date=dt.datetime(2024, 12, 31),\n",
    "    json_file=\"../data/latvia.geojson\",\n",
    "    frequency=\"monthly\",\n",
    "    resolution=\"0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_bbox(request_id, variables, start_date, end_date, north, south, east, west, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : datetime, start date of the data\n",
    "# end_date : datetime, end date of the data\n",
    "# north : float, northern boundary of the bounding box\n",
    "# south : float, southern boundary of the bounding box\n",
    "# east : float, eastern boundary of the bounding box\n",
    "# west : float, western boundary of the bounding box\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING ERA5 BBOX PROCESSING\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Bounding Box: N:30, S:20, E:80, W:70\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Bounding Box Validation ---\n",
      "✓ Bounding box coordinates validated successfully\n",
      "  Area: 10.0000° × 10.0000°\n",
      "\n",
      "--- Creating GeoJSON from Bounding Box ---\n",
      "✓ GeoJSON created successfully\n",
      "✓ Created temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "\n",
      "--- Delegating to Main Processing Function ---\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "Warning: GeoJSON doesn't have 'features' key, attempting to process anyway\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 30.0000°\n",
      "  South: 20.0000°\n",
      "  East:  80.0000°\n",
      "  West:  70.0000°\n",
      "  Area:  10.0000° × 10.0000°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 15\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C2\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/2\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-01-14\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test3_chunk1\n",
      "Date range: 2024-01-01 to 2024-01-14\n",
      "Area: North: 30, West: 70, South: 20, East: 80\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:52:26,337 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:52:27,183 INFO Request ID is 872c4878-2d0d-4982-9a16-de2e1d5822ae\n",
      "2025-05-29 22:52:27,410 INFO status has been updated to accepted\n",
      "2025-05-29 22:52:41,723 INFO status has been updated to running\n",
      "2025-05-29 22:53:18,384 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test3_chunk1.zip\n",
      "  ✓ Download completed: test3_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test3_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test3_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test3_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1681 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.01 seconds\n",
      "  - Points inside: 1521\n",
      "  - Points outside: 160\n",
      "  - Percentage inside: 90.48%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 564816 rows\n",
      "  ✓ Created lookup set with 1521 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 564816 to 511056 rows\n",
      "✓ Dataset filtering completed in 0.36 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.37 seconds\n",
      "Final DataFrame shape: (511056, 7)\n",
      "Rows in final dataset: 511056\n",
      "  ✓ Filtered data shape: (511056, 7)\n",
      "  ✓ Chunk completed in 59.64 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/2\n",
      "==================================================\n",
      "Processing: 2024-01-15 to 2024-01-15\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test3_chunk2\n",
      "Date range: 2024-01-15 to 2024-01-15\n",
      "Area: North: 30, West: 70, South: 20, East: 80\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:53:32,105 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-29 22:53:33,090 INFO Request ID is 42ccc1b7-9318-4e59-a9f8-7fe54242ad33\n",
      "2025-05-29 22:53:33,253 INFO status has been updated to accepted\n",
      "2025-05-29 22:53:55,452 INFO status has been updated to running\n",
      "2025-05-29 22:54:07,128 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test3_chunk2.zip\n",
      "  ✓ Download completed: test3_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test3_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test3_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test3_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1681 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.01 seconds\n",
      "  - Points inside: 1521\n",
      "  - Points outside: 160\n",
      "  - Percentage inside: 90.48%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 40344 rows\n",
      "  ✓ Created lookup set with 1521 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 40344 to 36504 rows\n",
      "✓ Dataset filtering completed in 0.03 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.04 seconds\n",
      "Final DataFrame shape: (36504, 7)\n",
      "Rows in final dataset: 36504\n",
      "  ✓ Filtered data shape: (36504, 7)\n",
      "  ✓ Chunk completed in 40.22 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (547560, 7)\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "✓ Aggregation completed in 0.12 seconds\n",
      "✓ Aggregated data shape: (15, 6)\n",
      "✓ Unique lat/long combinations: 1521\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test3_output\n",
      "✓ Aggregated data exported to: test3_output\\test3_daily_data.csv\n",
      "✓ Unique lat/longs exported to: test3_output\\test3_unique_latlongs.csv\n",
      "✓ Raw data exported to: test3_output\\test3_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 107.08 seconds\n",
      "Final dataset shape: (15, 6)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month  day        date\n",
      "0  0.000064  289.454437  2024      1    1  2024-01-01\n",
      "1  0.000165  288.761688  2024      1    2  2024-01-02\n",
      "2  0.000456  288.610870  2024      1    3  2024-01-03\n",
      "3  0.000360  288.144958  2024      1    4  2024-01-04\n",
      "4  0.000638  288.382355  2024      1    5  2024-01-05\n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ERA5 BBOX PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "--- Cleanup ---\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "✓ Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_bbox(\n",
    "    request_id=\"test3\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=dt.datetime(2024, 1, 1),\n",
    "    end_date=dt.datetime(2024, 1, 15),\n",
    "    north = 30,\n",
    "    south = 20,\n",
    "    east = 80,\n",
    "    west = 70,\n",
    "    frequency=\"daily\",\n",
    "    resolution=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
