{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_geojson(request_id, variables, start_date, end_date, json_file, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : string, start date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# end_date : string, end date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# json_file : str, path to the geojson file containing the area of interest\n",
    "# dataset_type : str, type of dataset (single or pressure, for single level or pressure level datasets), optional (single by default)\n",
    "# pressure_levels : list, list of strings of pressure levels to download (e.g., [\"1000\", \"925\", \"850\"]), optional (empty by default)\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/india.json\n",
      "Using GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test\n",
      "Variables: ['2m_temperature', 'total_precipitation', 'surface_pressure', '2m_dewpoint_temperature']\n",
      "Date Range: 2023-01-01 to 2023-01-31\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "\u001b[0;32m✓ Successfully loaded GeoJSON file with utf-8 encoding\u001b[0m\n",
      "\u001b[0;32m✓ GeoJSON contains 1 feature(s)\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 31\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 3 chunks C2\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-01 to 2023-01-14\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:23:20,956 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:23:21,622 INFO Request ID is 60cee10b-bbf0-425f-a151-13a71807237a\n",
      "2025-07-06 19:23:21,792 INFO status has been updated to accepted\n",
      "2025-07-06 19:23:30,947 INFO status has been updated to running\n",
      "2025-07-06 19:23:36,183 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:392: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.47 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.93 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.43 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  \u001b[0;32m✓ Filtered data shape: (1493856, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 30.04 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-15 to 2023-01-28\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:24:00,643 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:24:01,357 INFO Request ID is bfc6849a-d240-451b-9234-65431586c49e\n",
      "2025-07-06 19:24:01,505 INFO status has been updated to accepted\n",
      "2025-07-06 19:24:06,945 INFO status has been updated to running\n",
      "2025-07-06 19:24:10,474 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:392: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.51 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.63 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.14 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  \u001b[0;32m✓ Filtered data shape: (1493856, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 24.46 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 3/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-29 to 2023-01-31\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:24:35,133 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:24:35,938 INFO Request ID is cc38413d-22e4-4cb4-8faf-7fec522b9594\n",
      "2025-07-06 19:24:36,107 INFO status has been updated to accepted\n",
      "2025-07-06 19:24:41,476 INFO status has been updated to running\n",
      "2025-07-06 19:24:50,282 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:376: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:392: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3\\data_stream-oper_stepType-accum.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.45 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 935064 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 935064 to 320112 rows\n",
      "✓ Dataset filtering completed in 0.56 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.02 seconds\n",
      "Final DataFrame shape: (320112, 9)\n",
      "Rows in final dataset: 320112\n",
      "  \u001b[0;32m✓ Filtered data shape: (320112, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 20.62 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (3307824, 9)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m', 'sp', 'd2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.62 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (31, 8)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 4446\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test_output\\test_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test_output\\test_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test_output\\test_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 112.36 seconds\n",
      "Final dataset shape: (31, 8)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m            sp         d2m  year  month  day  \\\n",
      "0  0.000155  289.156586  95582.187500  283.245087  2023      1    1   \n",
      "1  0.000184  288.782196  95621.062500  283.129059  2023      1    2   \n",
      "2  0.000141  288.308716  95639.898438  282.865387  2023      1    3   \n",
      "3  0.000200  287.533905  95647.882812  281.997925  2023      1    4   \n",
      "4  0.000143  287.359467  95756.250000  281.820160  2023      1    5   \n",
      "\n",
      "         date  \n",
      "0  2023-01-01  \n",
      "1  2023-01-02  \n",
      "2  2023-01-03  \n",
      "3  2023-01-04  \n",
      "4  2023-01-05  \n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1.zip\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2.zip\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3.zip\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk1\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk2\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_chunk3\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test\",\n",
    "    variables=[\n",
    "        \"2m_temperature\",\n",
    "        \"total_precipitation\",\n",
    "        \"surface_pressure\",\n",
    "        \"2m_dewpoint_temperature\",\n",
    "    ],\n",
    "    start_date=\"2023-1-1\",\n",
    "    end_date=\"2023-1-31\", \n",
    "    json_file=\"../data/india.json\",\n",
    "    dataset_type=\"single\",\n",
    "    frequency=\"daily\",\n",
    "    resolution=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/latvia.geojson\n",
      "Using GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test2\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-12-31\n",
      "Frequency: monthly\n",
      "Resolution: 0.1°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "\u001b[0;32m✓ Successfully loaded GeoJSON file with utf-8 encoding\u001b[0m\n",
      "\u001b[0;32m✓ GeoJSON contains 1 feature(s)\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 58.0856°\n",
      "  South: 55.6776°\n",
      "  East:  28.2431°\n",
      "  West:  20.9537°\n",
      "  Area:  7.2894° × 2.4080°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 12\n",
      "Max months per chunk: 100\n",
      "Needs chunking: False\n",
      "Processing as a single chunk (12 months)... C3\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:51:17,923 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:51:19,142 INFO Request ID is d48fb6c1-8499-4163-a1cd-563299522e9a\n",
      "2025-07-06 19:51:19,340 INFO status has been updated to accepted\n",
      "2025-07-06 19:51:28,213 INFO status has been updated to running\n",
      "2025-07-06 19:51:52,796 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:468: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:468: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:480: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"{Colors.GREEN}✓ Merged dataset shape: {dict(merged_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2.zip\u001b[0m\n",
      "→ Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2\\data_stream-moda_stepType-avgad.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2\\data_stream-moda_stepType-avgua.nc\n",
      "\u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "→ Processing NetCDF files...\n",
      "  Processing file 1/2: data_stream-moda_stepType-avgad.nc\n",
      "  ✓ Loaded dataset with shape: {'valid_time': 12, 'latitude': 25, 'longitude': 73}\n",
      "  Processing file 2/2: data_stream-moda_stepType-avgua.nc\n",
      "  ✓ Loaded dataset with shape: {'valid_time': 12, 'latitude': 25, 'longitude': 73}\n",
      "→ Merging datasets...\n",
      "\u001b[0;32m✓ Merged dataset shape: {'valid_time': 24, 'latitude': 25, 'longitude': 73}\u001b[0m\n",
      "→ Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.20 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 43800 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 43800 to 23088 rows\n",
      "✓ Dataset filtering completed in 0.03 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.22 seconds\n",
      "Final DataFrame shape: (23088, 7)\n",
      "Rows in final dataset: 23088\n",
      "\u001b[0;32m✓ Filtered data shape: (23088, 7)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (monthly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to monthly frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.02 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (12, 4)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 962\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test2_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test2_output\\test2_monthly_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test2_output\\test2_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test2_output\\test2_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 38.72 seconds\n",
      "Final dataset shape: (12, 4)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month\n",
      "0  0.001762  267.574188  2024      1\n",
      "1  0.002134  273.594452  2024      2\n",
      "2  0.000817  276.579803  2024      3\n",
      "3  0.002484  280.693359  2024      4\n",
      "4  0.000811  287.699432  2024      5\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2.zip\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test2\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=\"2024-1-1\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    json_file=\"../data/latvia.geojson\",\n",
    "    dataset_type=\"single\",\n",
    "    frequency=\"monthly\",\n",
    "    resolution=\"0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_bbox(request_id, variables, start_date, end_date, north, south, east, west, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : string, start date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# end_date : string, end date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# north : float, northern boundary of the bounding box\n",
    "# south : float, southern boundary of the bounding box\n",
    "# east : float, eastern boundary of the bounding box\n",
    "# west : float, western boundary of the bounding box\n",
    "# dataset_type : str, type of dataset (single or pressure, for single level or pressure level datasets), optional (single by default)\n",
    "# pressure_levels : list, list of strings of pressure levels to download (e.g., [\"1000\", \"925\", \"850\"]), optional (empty by default)\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING ERA5 SINGLE LEVEL BBOX PROCESSING\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Bounding Box: N:30, S:20, E:80, W:70\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Bounding Box Validation ---\n",
      "✓ Bounding box coordinates validated successfully\n",
      "  Area: 10.0000° × 10.0000°\n",
      "\n",
      "--- Creating GeoJSON from Bounding Box ---\n",
      "✓ GeoJSON created successfully\n",
      "✓ Created temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 30.0000°\n",
      "  South: 20.0000°\n",
      "  East:  80.0000°\n",
      "  West:  70.0000°\n",
      "  Area:  10.0000° × 10.0000°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 15\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C2\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-01-14\n",
      "  → Downloading ERA5 data...\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:56:19,855 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:56:20,719 INFO Request ID is 73a5e83a-a60a-4d8d-9478-05564a265b4e\n",
      "2025-07-06 19:56:20,915 INFO status has been updated to accepted\n",
      "2025-07-06 19:56:29,775 INFO status has been updated to running\n",
      "2025-07-06 19:57:11,591 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:881: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:881: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:897: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1.zip\u001b[0m\n",
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\u001b[0m\n",
      "  \u001b[0;32m✓ Dataframe shape: (564816, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 56.94 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-15 to 2024-01-15\n",
      "  → Downloading ERA5 data...\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 19:57:26,675 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 19:57:27,322 INFO Request ID is 30cf795b-1c9e-4e70-b606-c857ecb5cb22\n",
      "2025-07-06 19:57:27,494 INFO status has been updated to accepted\n",
      "2025-07-06 19:57:32,817 INFO status has been updated to running\n",
      "2025-07-06 19:57:41,655 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:881: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:881: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:897: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2.zip\u001b[0m\n",
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\u001b[0m\n",
      "  \u001b[0;32m✓ Dataframe shape: (40344, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 19.42 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (605160, 7)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.11 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (15, 6)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 1681\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test3_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test3_output\\test3_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test3_output\\test3_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test3_output\\test3_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 88.62 seconds\n",
      "Final dataset shape: (15, 6)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month  day        date\n",
      "0  0.000059  289.480377  2024      1    1  2024-01-01\n",
      "1  0.000158  288.823975  2024      1    2  2024-01-02\n",
      "2  0.000437  288.676178  2024      1    3  2024-01-03\n",
      "3  0.000333  288.236481  2024      1    4  2024-01-04\n",
      "4  0.000649  288.469452  2024      1    5  2024-01-05\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ERA5 SINGLE LEVEL BBOX PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "--- Cleanup ---\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "✓ Cleanup completed\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1.zip\n",
      "Removing ERA5 zip file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2.zip\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk1\n",
      "Removing extraction directory: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_chunk2\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_bbox(\n",
    "    request_id=\"test3\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=\"2024-01-1\",\n",
    "    end_date=\"2024-01-15\",\n",
    "    north = 30,\n",
    "    south = 20,\n",
    "    east = 80,\n",
    "    west = 70,\n",
    "    frequency=\"daily\",\n",
    "    resolution=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING ERA5 PRESSURE LEVEL BBOX PROCESSING\n",
      "============================================================\n",
      "Request ID: test4\n",
      "Variables: ['temperature', 'relative_humidity']\n",
      "Pressure Levels: ['1000']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Bounding Box: N:30, S:20, E:80, W:70\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Bounding Box Validation ---\n",
      "✓ Bounding box coordinates validated successfully\n",
      "  Area: 10.0000° × 10.0000°\n",
      "\n",
      "--- Creating GeoJSON from Bounding Box ---\n",
      "✓ GeoJSON created successfully\n",
      "✓ Created temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 PRESSURE LEVEL PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test4\n",
      "Variables: ['temperature', 'relative_humidity']\n",
      "Pressure Levels: ['1000']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 30.0000°\n",
      "  South: 20.0000°\n",
      "  East:  80.0000°\n",
      "  West:  70.0000°\n",
      "  Area:  10.0000° × 10.0000°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 15\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-01-14\n",
      "  → Downloading ERA5 pressure level data...\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 20:05:19,801 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 20:05:20,188 INFO Request ID is fd033ea6-b44a-4719-ba02-b39aae5e473c\n",
      "2025-07-06 20:05:20,353 INFO status has been updated to accepted\n",
      "2025-07-06 20:05:25,810 INFO status has been updated to running\n",
      "2025-07-06 20:05:29,367 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1601: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk1.nc\u001b[0m\n",
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk1.nc\u001b[0m\n",
      "  → Processing NetCDF file...\n",
      "  ✓ Loaded dataset with shape: {'valid_time': 336, 'pressure_level': 1, 'latitude': 41, 'longitude': 41}\n",
      "  \u001b[0;32m✓ Dataframe shape: (564816, 8)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 26.89 seconds\u001b[0m\n",
      "  → Waiting 10 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-15 to 2024-01-15\n",
      "  → Downloading ERA5 pressure level data...\n",
      "  → Downloading ERA5 data (attempt 1/6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 20:05:56,646 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-06 20:05:57,063 INFO Request ID is f28d74f2-86ef-49de-ac80-1a9774fc11dd\n",
      "2025-07-06 20:05:57,308 INFO status has been updated to accepted\n",
      "2025-07-06 20:06:06,295 INFO status has been updated to running\n",
      "2025-07-06 20:06:11,534 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1601: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk2.nc\u001b[0m\n",
      "  \u001b[0;32m✓ Download completed: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk2.nc\u001b[0m\n",
      "  → Processing NetCDF file...\n",
      "  ✓ Loaded dataset with shape: {'valid_time': 24, 'pressure_level': 1, 'latitude': 41, 'longitude': 41}\n",
      "  \u001b[0;32m✓ Dataframe shape: (40344, 8)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 18.36 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (605160, 8)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating pressure level data to daily frequency...\n",
      "Variables to average: ['t', 'r']\n",
      "Including pressure_level in aggregation groups\n",
      "             t          r  pressure_level  year  month  day\n",
      "0   292.125336  72.452385          1000.0  2024      1    1\n",
      "1   291.697723  72.968575          1000.0  2024      1    2\n",
      "2   291.161835  74.767265          1000.0  2024      1    3\n",
      "3   291.061493  74.192192          1000.0  2024      1    4\n",
      "4   290.956696  73.344673          1000.0  2024      1    5\n",
      "5   290.726990  72.663681          1000.0  2024      1    6\n",
      "6   290.992035  73.618111          1000.0  2024      1    7\n",
      "7   292.213470  73.400780          1000.0  2024      1    8\n",
      "8   291.508179  72.738716          1000.0  2024      1    9\n",
      "9   291.173004  72.498398          1000.0  2024      1   10\n",
      "10  292.272430  68.849129          1000.0  2024      1   11\n",
      "11  293.743317  64.434761          1000.0  2024      1   12\n",
      "12  294.506287  62.466526          1000.0  2024      1   13\n",
      "13  293.213074  62.304012          1000.0  2024      1   14\n",
      "14  292.271759  61.974461          1000.0  2024      1   15\n",
      "\u001b[0;32m✓ Aggregation completed in 0.06 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (15, 6)\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test4_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test4_output\\test4_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test4_output\\test4_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test4_output\\test4_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 57.38 seconds\n",
      "Final dataset shape: (15, 6)\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 PRESSURE LEVEL PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ERA5 PRESSURE LEVEL BBOX PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "--- Cleanup ---\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_temp_geojson.json\n",
      "✓ Cleanup completed\n",
      "Removing ERA5 nc file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk1.nc\n",
      "Removing ERA5 nc file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_chunk2.nc\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_bbox(\n",
    "    request_id=\"test4\",\n",
    "    variables=[\n",
    "        \"temperature\", \n",
    "        \"relative_humidity\"\n",
    "    ],\n",
    "    start_date=\"2024-01-1\",\n",
    "    end_date=\"2024-01-15\",\n",
    "    north = 30,\n",
    "    south = 20,\n",
    "    east = 80,\n",
    "    west = 70,\n",
    "    dataset_type=\"pressure\",\n",
    "    pressure_levels=[\"1000\"],\n",
    "    frequency=\"daily\",\n",
    "    resolution=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
