{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_geojson(request_id, variables, start_date, end_date, json_file, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : string, start date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# end_date : string, end date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# json_file : str, path to the geojson file containing the area of interest\n",
    "# dataset_type : str, type of dataset (single or pressure, for single level or pressure level datasets), optional (single by default)\n",
    "# pressure_levels : list, list of strings of pressure levels to download (e.g., [\"1000\", \"925\", \"850\"]), optional (empty by default)\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/india.json\n",
      "Using GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test\n",
      "Variables: ['2m_temperature', 'total_precipitation', 'surface_pressure', '2m_dewpoint_temperature']\n",
      "Date Range: 2023-01-01 to 2023-01-31\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "\u001b[0;32m✓ Successfully loaded GeoJSON file with utf-8 encoding\u001b[0m\n",
      "\u001b[0;32m✓ GeoJSON contains 1 feature(s)\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 31\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 3 chunks C2\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-01 to 2023-01-14\n",
      "  → Downloading ERA5 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:05:55,803 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-07-01 16:05:55,804 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-01 16:05:55,804 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-01 16:05:56,700 INFO Request ID is e8238e8e-a599-4ccf-8309-533befb317fb\n",
      "2025-07-01 16:05:57,007 INFO status has been updated to accepted\n",
      "2025-07-01 16:06:05,916 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: test_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:354: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.51 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.66 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.18 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  \u001b[0;32m✓ Filtered data shape: (1493856, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 34.37 seconds\u001b[0m\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-15 to 2023-01-28\n",
      "  → Downloading ERA5 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:06:35,101 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-07-01 16:06:35,103 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-01 16:06:35,104 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-01 16:06:36,225 INFO Request ID is c2354300-97e9-4048-b42e-b501ee32540f\n",
      "2025-07-01 16:06:36,635 INFO status has been updated to accepted\n",
      "2025-07-01 16:06:42,473 INFO status has been updated to running\n",
      "2025-07-01 16:06:46,057 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:354: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: test_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.52 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.70 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.22 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  \u001b[0;32m✓ Filtered data shape: (1493856, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 35.09 seconds\u001b[0m\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 3/3\u001b[0m\n",
      "==================================================\n",
      "Processing: 2023-01-29 to 2023-01-31\n",
      "  → Downloading ERA5 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:07:15,066 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-07-01 16:07:15,068 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-01 16:07:15,069 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-01 16:07:15,855 INFO Request ID is 572a7972-7974-4b97-be24-9509e3572e6f\n",
      "2025-07-01 16:07:16,060 INFO status has been updated to accepted\n",
      "2025-07-01 16:07:30,294 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:338: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:354: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: test_chunk3.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test_chunk3\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test_chunk3\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.48 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 935064 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 935064 to 320112 rows\n",
      "✓ Dataset filtering completed in 0.57 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.05 seconds\n",
      "Final DataFrame shape: (320112, 9)\n",
      "Rows in final dataset: 320112\n",
      "  \u001b[0;32m✓ Filtered data shape: (320112, 9)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 28.81 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (3307824, 9)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m', 'sp', 'd2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.62 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (31, 8)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 4446\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test_output\\test_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test_output\\test_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test_output\\test_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 125.53 seconds\n",
      "Final dataset shape: (31, 8)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m            sp         d2m  year  month  day  \\\n",
      "0  0.000155  289.156586  95582.187500  283.245087  2023      1    1   \n",
      "1  0.000184  288.782196  95621.062500  283.129059  2023      1    2   \n",
      "2  0.000141  288.308716  95639.898438  282.865387  2023      1    3   \n",
      "3  0.000200  287.533905  95647.882812  281.997925  2023      1    4   \n",
      "4  0.000143  287.359467  95756.250000  281.820160  2023      1    5   \n",
      "\n",
      "         date  \n",
      "0  2023-01-01  \n",
      "1  2023-01-02  \n",
      "2  2023-01-03  \n",
      "3  2023-01-04  \n",
      "4  2023-01-05  \n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test_temp_geojson.json\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test\",\n",
    "    variables=[\n",
    "        \"2m_temperature\",\n",
    "        \"total_precipitation\",\n",
    "        \"surface_pressure\",\n",
    "        \"2m_dewpoint_temperature\",\n",
    "    ],\n",
    "    start_date=\"2023-1-1\",\n",
    "    end_date=\"2023-1-31\", \n",
    "    json_file=\"../data/india.json\",\n",
    "    dataset_type=\"single\",\n",
    "    frequency=\"daily\",\n",
    "    resolution=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/latvia.geojson\n",
      "Using GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test2\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-12-31\n",
      "Frequency: monthly\n",
      "Resolution: 0.1°\n",
      "GeoJSON File: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "\u001b[0;32m✓ Successfully loaded GeoJSON file with utf-8 encoding\u001b[0m\n",
      "\u001b[0;32m✓ GeoJSON contains 1 feature(s)\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 58.0856°\n",
      "  South: 55.6776°\n",
      "  East:  28.2431°\n",
      "  West:  20.9537°\n",
      "  Area:  7.2894° × 2.4080°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 12\n",
      "Max months per chunk: 10\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C1\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-10-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk1\n",
      "Date range: 2024-01-01 to 2024-10-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 17:15:05,739 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-28 17:15:05,740 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-28 17:15:05,740 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-28 17:15:05,741 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-28 17:15:06,229 INFO Request ID is 8b98b358-79c2-456b-a1b2-f2440241e20b\n",
      "2025-06-28 17:15:06,453 INFO status has been updated to accepted\n",
      "2025-06-28 17:15:15,761 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1695: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1695: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1712: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk1.zip\n",
      "  \u001b[0;32m✓ Download completed: test2_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test2_chunk1\\data_stream-moda_stepType-avgad.nc\n",
      "  - d:\\eranest\\notebooks\\test2_chunk1\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-moda_stepType-avgad.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 20, 'latitude': 25, 'longitude': 73}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.22 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 36500 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 36500 to 19240 rows\n",
      "✓ Dataset filtering completed in 0.02 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.24 seconds\n",
      "Final DataFrame shape: (19240, 7)\n",
      "Rows in final dataset: 19240\n",
      "  \u001b[0;32m✓ Filtered data shape: (19240, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 14.57 seconds\u001b[0m\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-11-01 to 2024-12-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk2\n",
      "Date range: 2024-11-01 to 2024-12-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 17:15:23,924 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-28 17:15:23,925 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-28 17:15:23,926 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-28 17:15:23,926 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-28 17:15:24,547 INFO Request ID is c72e1300-8745-4c3f-a285-d54374b53f6a\n",
      "2025-06-28 17:15:24,759 INFO status has been updated to accepted\n",
      "2025-06-28 17:15:38,918 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1695: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1695: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:1712: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk2.zip\n",
      "  \u001b[0;32m✓ Download completed: test2_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test2_chunk2\\data_stream-moda_stepType-avgad.nc\n",
      "  - d:\\eranest\\notebooks\\test2_chunk2\\data_stream-moda_stepType-avgua.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-moda_stepType-avgad.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_stream-moda_stepType-avgua.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 4, 'latitude': 25, 'longitude': 73}\u001b[0m\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.20 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 7300 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 7300 to 3848 rows\n",
      "✓ Dataset filtering completed in 0.01 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.21 seconds\n",
      "Final DataFrame shape: (3848, 7)\n",
      "Rows in final dataset: 3848\n",
      "  \u001b[0;32m✓ Filtered data shape: (3848, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 18.13 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (23088, 7)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (monthly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to monthly frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.01 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (12, 4)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 962\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test2_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test2_output\\test2_monthly_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test2_output\\test2_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test2_output\\test2_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 37.83 seconds\n",
      "Final dataset shape: (12, 4)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month\n",
      "0  0.001762  267.574188  2024      1\n",
      "1  0.002134  273.594452  2024      2\n",
      "2  0.000817  276.579803  2024      3\n",
      "3  0.002484  280.693359  2024      4\n",
      "4  0.000811  287.699432  2024      5\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test2_temp_geojson.json\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_geojson(\n",
    "    request_id=\"test2\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=\"2024-1-1\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    json_file=\"../data/latvia.geojson\",\n",
    "    dataset_type=\"single\",\n",
    "    frequency=\"monthly\",\n",
    "    resolution=\"0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify_bbox(request_id, variables, start_date, end_date, north, south, east, west, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : string, start date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# end_date : string, end date of the data ('YYYY-M-D' or 'YYYY-MM-DD')\n",
    "# north : float, northern boundary of the bounding box\n",
    "# south : float, southern boundary of the bounding box\n",
    "# east : float, eastern boundary of the bounding box\n",
    "# west : float, western boundary of the bounding box\n",
    "# dataset_type : str, type of dataset (single or pressure, for single level or pressure level datasets), optional (single by default)\n",
    "# pressure_levels : list, list of strings of pressure levels to download (e.g., [\"1000\", \"925\", \"850\"]), optional (empty by default)\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING ERA5 SINGLE LEVEL BBOX PROCESSING\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Bounding Box: N:30, S:20, E:80, W:70\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Bounding Box Validation ---\n",
      "✓ Bounding box coordinates validated successfully\n",
      "  Area: 10.0000° × 10.0000°\n",
      "\n",
      "--- Creating GeoJSON from Bounding Box ---\n",
      "✓ GeoJSON created successfully\n",
      "✓ Created temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 DATA PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test3\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 30.0000°\n",
      "  South: 20.0000°\n",
      "  East:  80.0000°\n",
      "  West:  70.0000°\n",
      "  Area:  10.0000° × 10.0000°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 15\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C2\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-01-14\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test3_chunk1\n",
      "Date range: 2024-01-01 to 2024-01-14\n",
      "Area: North: 30, West: 70, South: 20, East: 80\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 17:29:38,989 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-28 17:29:38,990 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-28 17:29:38,991 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-28 17:29:38,991 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-28 17:29:39,760 INFO Request ID is 28749bbd-1440-4000-aace-eb75a7e8be32\n",
      "2025-06-28 17:29:39,928 INFO status has been updated to accepted\n",
      "2025-06-28 17:30:01,678 INFO status has been updated to running\n",
      "2025-06-28 17:30:30,650 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:2333: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:2333: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:2349: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test3_chunk1.zip\n",
      "  \u001b[0;32m✓ Download completed: test3_chunk1.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test3_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test3_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test3_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 336, 'latitude': 41, 'longitude': 41}\u001b[0m\n",
      "  \u001b[0;32m✓ Dataframe shape: (564816, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 55.58 seconds\u001b[0m\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-15 to 2024-01-15\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test3_chunk2\n",
      "Date range: 2024-01-15 to 2024-01-15\n",
      "Area: North: 30, West: 70, South: 20, East: 80\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 17:30:39,640 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-28 17:30:39,642 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-28 17:30:39,642 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-28 17:30:39,643 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-28 17:30:40,332 INFO Request ID is 6f89a0c4-b90d-47e5-aac5-5ed8dd384e12\n",
      "2025-06-28 17:30:40,597 INFO status has been updated to accepted\n",
      "2025-06-28 17:30:49,384 INFO status has been updated to running\n",
      "2025-06-28 17:31:02,364 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:2333: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:2333: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "D:\\eranest\\eranest\\core.py:2349: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  {Colors.GREEN}✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}{Colors.RESET}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test3_chunk2.zip\n",
      "  \u001b[0;32m✓ Download completed: test3_chunk2.zip\u001b[0m\n",
      "  → Extracting files...\n",
      "Extracting zip file: test3_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\eranest\\notebooks\\test3_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\eranest\\notebooks\\test3_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  \u001b[0;32m✓ Extracted 2 files\u001b[0m\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\n",
      "  → Merging datasets...\n",
      "  \u001b[0;32m✓ Merged dataset shape: {'valid_time': 24, 'latitude': 41, 'longitude': 41}\u001b[0m\n",
      "  \u001b[0;32m✓ Dataframe shape: (40344, 7)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 25.75 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (605160, 7)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "\u001b[0;32m✓ Aggregation completed in 0.12 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (15, 6)\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/long combinations: 1681\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test3_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test3_output\\test3_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test3_output\\test3_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test3_output\\test3_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 88.71 seconds\n",
      "Final dataset shape: (15, 6)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month  day        date\n",
      "0  0.000059  289.480377  2024      1    1  2024-01-01\n",
      "1  0.000158  288.823975  2024      1    2  2024-01-02\n",
      "2  0.000437  288.676178  2024      1    3  2024-01-03\n",
      "3  0.000333  288.236481  2024      1    4  2024-01-04\n",
      "4  0.000649  288.469452  2024      1    5  2024-01-05\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ERA5 SINGLE LEVEL BBOX PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "--- Cleanup ---\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test3_temp_geojson.json\n",
      "✓ Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_bbox(\n",
    "    request_id=\"test3\",\n",
    "    variables=[\n",
    "        \"2m_temperature\", \n",
    "        \"total_precipitation\"\n",
    "    ],\n",
    "    start_date=\"2024-01-1\",\n",
    "    end_date=\"2024-01-15\",\n",
    "    north = 30,\n",
    "    south = 20,\n",
    "    east = 80,\n",
    "    west = 70,\n",
    "    frequency=\"daily\",\n",
    "    resolution=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING ERA5 PRESSURE LEVEL BBOX PROCESSING\n",
      "============================================================\n",
      "Request ID: test4\n",
      "Variables: ['temperature', 'relative_humidity']\n",
      "Pressure Levels: ['1000']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Bounding Box: N:30, S:20, E:80, W:70\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Bounding Box Validation ---\n",
      "✓ Bounding box coordinates validated successfully\n",
      "  Area: 10.0000° × 10.0000°\n",
      "\n",
      "--- Creating GeoJSON from Bounding Box ---\n",
      "✓ GeoJSON created successfully\n",
      "✓ Created temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_temp_geojson.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mSTARTING ERA5 PRESSURE LEVEL PROCESSING\u001b[0m\n",
      "============================================================\n",
      "Request ID: test4\n",
      "Variables: ['temperature', 'relative_humidity']\n",
      "Pressure Levels: ['1000']\n",
      "Date Range: 2024-01-01 to 2024-01-15\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "\n",
      "--- Input Validation ---\n",
      "\u001b[0;32m✓ All inputs validated successfully\u001b[0m\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "\u001b[0;32m✓ Bounding Box calculated:\u001b[0m\n",
      "  North: 30.0000°\n",
      "  South: 20.0000°\n",
      "  East:  80.0000°\n",
      "  West:  70.0000°\n",
      "  Area:  10.0000° × 10.0000°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 15\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 1/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-01-14\n",
      "  → Downloading ERA5 pressure level data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 17:21:06,060 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-02 17:21:06,061 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-02 17:21:06,582 INFO Request ID is e85b2203-f35e-421e-b910-45cfe5cf8ec6\n",
      "2025-07-02 17:21:06,738 INFO status has been updated to accepted\n",
      "2025-07-02 17:21:12,032 INFO status has been updated to running\n",
      "2025-07-02 17:22:23,146 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: test4_chunk1.nc\u001b[0m\n",
      "  → Processing NetCDF file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\eranest\\eranest\\core.py:1541: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Loaded dataset with shape: {'valid_time': 336, 'pressure_level': 1, 'latitude': 41, 'longitude': 41}\n",
      "  \u001b[0;32m✓ Dataframe shape: (564816, 8)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 83.81 seconds\u001b[0m\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "\u001b[0;36mCHUNK 2/2\u001b[0m\n",
      "==================================================\n",
      "Processing: 2024-01-15 to 2024-01-15\n",
      "  → Downloading ERA5 pressure level data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 17:22:33,931 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-02 17:22:33,934 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-02 17:22:34,312 INFO Request ID is ecd96dde-9d5b-4858-b586-39566e44e873\n",
      "2025-07-02 17:22:34,571 INFO status has been updated to accepted\n",
      "2025-07-02 17:22:43,436 INFO status has been updated to running\n",
      "2025-07-02 17:22:49,097 INFO status has been updated to successful\n",
      "D:\\eranest\\eranest\\core.py:1541: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[0;32m✓ Download completed: test4_chunk2.nc\u001b[0m\n",
      "  → Processing NetCDF file...\n",
      "  ✓ Loaded dataset with shape: {'valid_time': 24, 'pressure_level': 1, 'latitude': 41, 'longitude': 41}\n",
      "  \u001b[0;32m✓ Dataframe shape: (40344, 8)\u001b[0m\n",
      "  \u001b[0;32m✓ Chunk completed in 22.79 seconds\u001b[0m\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "\u001b[0;32m✓ Combined data shape: (605160, 8)\u001b[0m\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating pressure level data to daily frequency...\n",
      "Variables to average: ['t', 'r']\n",
      "Including pressure_level in aggregation groups\n",
      "             t          r  pressure_level  year  month  day\n",
      "0   292.125336  72.452385          1000.0  2024      1    1\n",
      "1   291.697723  72.968575          1000.0  2024      1    2\n",
      "2   291.161835  74.767265          1000.0  2024      1    3\n",
      "3   291.061493  74.192192          1000.0  2024      1    4\n",
      "4   290.956696  73.344673          1000.0  2024      1    5\n",
      "5   290.726990  72.663681          1000.0  2024      1    6\n",
      "6   290.992035  73.618111          1000.0  2024      1    7\n",
      "7   292.213470  73.400780          1000.0  2024      1    8\n",
      "8   291.508179  72.738716          1000.0  2024      1    9\n",
      "9   291.173004  72.498398          1000.0  2024      1   10\n",
      "10  292.272430  68.849129          1000.0  2024      1   11\n",
      "11  293.743317  64.434761          1000.0  2024      1   12\n",
      "12  294.506287  62.466526          1000.0  2024      1   13\n",
      "13  293.213074  62.304012          1000.0  2024      1   14\n",
      "14  292.271759  61.974461          1000.0  2024      1   15\n",
      "\u001b[0;32m✓ Aggregation completed in 0.11 seconds\u001b[0m\n",
      "\u001b[0;32m✓ Aggregated data shape: (15, 6)\u001b[0m\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test4_output\n",
      "\u001b[0;32m✓ Aggregated data exported to: test4_output\\test4_daily_data.csv\u001b[0m\n",
      "\u001b[0;32m✓ Unique lat/longs exported to: test4_output\\test4_unique_latlongs.csv\u001b[0m\n",
      "\u001b[0;32m✓ Raw data exported to: test4_output\\test4_raw_data.csv\u001b[0m\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 114.88 seconds\n",
      "Final dataset shape: (15, 6)\n",
      "\n",
      "============================================================\n",
      "\u001b[0;34mERA5 PRESSURE LEVEL PROCESSING COMPLETED SUCCESSFULLY\u001b[0m\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ERA5 PRESSURE LEVEL BBOX PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "--- Cleanup ---\n",
      "Removing temporary GeoJSON file: C:\\Users\\ATHARV~1\\AppData\\Local\\Temp\\test4_temp_geojson.json\n",
      "✓ Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify_bbox(\n",
    "    request_id=\"test4\",\n",
    "    variables=[\n",
    "        \"temperature\", \n",
    "        \"relative_humidity\"\n",
    "    ],\n",
    "    start_date=\"2024-01-1\",\n",
    "    end_date=\"2024-01-15\",\n",
    "    north = 30,\n",
    "    south = 20,\n",
    "    east = 80,\n",
    "    west = 70,\n",
    "    dataset_type=\"pressure\",\n",
    "    pressure_levels=[\"1000\"],\n",
    "    frequency=\"daily\",\n",
    "    resolution=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
