{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import eranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify(request_id, variables, start_date, end_date, json_file, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : datetime, start date of the data\n",
    "# end_date : datetime, end date of the data\n",
    "# json_file : str, path to the geojson file containing the area of interest\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/india.json\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test\n",
      "Variables: ['2m_temperature', 'total_precipitation', 'surface_pressure', '2m_dewpoint_temperature']\n",
      "Date Range: 2023-01-01 to 2023-01-31\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: ../data/india.json\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: ../data/india.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 31\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 3 chunks C2\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/3\n",
      "==================================================\n",
      "Processing: 2023-01-01 to 2023-01-14\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk1\n",
      "Date range: 2023-01-01 to 2023-01-14\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:56:10,913 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 22:56:10,915 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 22:56:11,661 INFO Request ID is e5328113-a911-4497-8003-de28a64298b3\n",
      "2025-05-26 22:56:11,953 INFO status has been updated to accepted\n",
      "2025-05-26 22:56:34,431 INFO status has been updated to successful\n",
      "                                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk1.zip\n",
      "  ✓ Download completed: test_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk1/data_stream-oper_stepType-accum.nc\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk1/data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.16 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.87 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.04 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 33.49 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/3\n",
      "==================================================\n",
      "Processing: 2023-01-15 to 2023-01-28\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk2\n",
      "Date range: 2023-01-15 to 2023-01-28\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:56:49,261 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 22:56:49,263 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 22:56:50,555 INFO Request ID is 0ba79ae4-1c0d-428d-8936-f3ec7a1ed50f\n",
      "2025-05-26 22:56:50,757 INFO status has been updated to accepted\n",
      "2025-05-26 22:57:13,270 INFO status has been updated to successful\n",
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk2.zip\n",
      "  ✓ Download completed: test_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk2/data_stream-oper_stepType-accum.nc\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk2/data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.05 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 3.93 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.99 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 34.07 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 3/3\n",
      "==================================================\n",
      "Processing: 2023-01-29 to 2023-01-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk3\n",
      "Date range: 2023-01-29 to 2023-01-31\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:57:28,286 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 22:57:28,287 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 22:57:29,264 INFO Request ID is a9250abb-5f95-4200-8e0e-a3c3feb0c531\n",
      "2025-05-26 22:57:29,585 INFO status has been updated to accepted\n",
      "2025-05-26 22:57:38,796 INFO status has been updated to running\n",
      "2025-05-26 22:57:44,144 INFO status has been updated to successful\n",
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1590: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1606: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk3.zip\n",
      "  ✓ Download completed: test_chunk3.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk3/data_stream-oper_stepType-accum.nc\n",
      "  - /Users/saket/github/eranest/notebooks/test_chunk3/data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.06 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 935064 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 935064 to 320112 rows\n",
      "✓ Dataset filtering completed in 0.51 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.58 seconds\n",
      "Final DataFrame shape: (320112, 9)\n",
      "Rows in final dataset: 320112\n",
      "  ✓ Filtered data shape: (320112, 9)\n",
      "  ✓ Chunk completed in 50.70 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (3307824, 9)\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m', 'sp', 'd2m']\n",
      "✓ Aggregation completed in 0.60 seconds\n",
      "✓ Aggregated data shape: (31, 8)\n",
      "✓ Unique lat/long combinations: 4446\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test_output\n",
      "✓ Aggregated data exported to: test_output/test_daily_data.csv\n",
      "✓ Unique lat/longs exported to: test_output/test_unique_latlongs.csv\n",
      "✓ Raw data exported to: test_output/test_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 145.14 seconds\n",
      "Final dataset shape: (31, 8)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m            sp         d2m  year  month  day  \\\n",
      "0  0.000155  289.156586  95582.187500  283.245087  2023      1    1   \n",
      "1  0.000184  288.782196  95621.062500  283.129059  2023      1    2   \n",
      "2  0.000141  288.308716  95639.898438  282.865387  2023      1    3   \n",
      "3  0.000200  287.533905  95647.882812  281.997925  2023      1    4   \n",
      "4  0.000143  287.359467  95756.250000  281.820160  2023      1    5   \n",
      "\n",
      "         date  \n",
      "0  2023-01-01  \n",
      "1  2023-01-02  \n",
      "2  2023-01-03  \n",
      "3  2023-01-04  \n",
      "4  2023-01-05  \n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify(\n",
    "    request_id=\"test\",\n",
    "    variables=[\n",
    "        \"2m_temperature\",\n",
    "        \"total_precipitation\",\n",
    "        \"surface_pressure\",\n",
    "        \"2m_dewpoint_temperature\",\n",
    "    ],\n",
    "    start_date=dt.datetime(2023, 1, 1),\n",
    "    end_date=dt.datetime(2023, 1, 31),\n",
    "    json_file=\"../data/india.json\",\n",
    "    frequency=\"daily\",\n",
    "    resolution=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: ../data/latvia.geojson\n",
      "✓ CDS API configuration is already set up and valid.\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test2\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-12-31\n",
      "Frequency: monthly\n",
      "Resolution: 0.1°\n",
      "GeoJSON File: ../data/latvia.geojson\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: ../data/latvia.geojson\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 58.0856°\n",
      "  South: 55.6776°\n",
      "  East:  28.2431°\n",
      "  West:  20.9537°\n",
      "  Area:  7.2894° × 2.4080°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 12\n",
      "Max months per chunk: 10\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C1\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/2\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-10-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk1\n",
      "Date range: 2024-01-01 to 2024-10-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:58:40,968 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 22:58:40,969 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 22:58:42,098 INFO Request ID is d75e9b45-bd62-4400-9937-5c229e8a4ac3\n",
      "2025-05-26 22:58:42,481 INFO status has been updated to accepted\n",
      "2025-05-26 22:58:56,946 INFO status has been updated to running\n",
      "2025-05-26 22:59:16,666 INFO status has been updated to successful\n",
      "/Users/saket/github/eranest/eranest/core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1495: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk1.zip\n",
      "  ✓ Download completed: test2_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - /Users/saket/github/eranest/notebooks/test2_chunk1/data_0.nc\n",
      "  - /Users/saket/github/eranest/notebooks/test2_chunk1/data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 20, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.01 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 36500 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 36500 to 19240 rows\n",
      "✓ Dataset filtering completed in 0.02 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.04 seconds\n",
      "Final DataFrame shape: (19240, 7)\n",
      "Rows in final dataset: 19240\n",
      "  ✓ Filtered data shape: (19240, 7)\n",
      "  ✓ Chunk completed in 43.53 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/2\n",
      "==================================================\n",
      "Processing: 2024-11-01 to 2024-12-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk2\n",
      "Date range: 2024-11-01 to 2024-12-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:59:24,498 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 22:59:24,499 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 22:59:25,387 INFO Request ID is 99db9761-26b3-44db-aebe-60c2f0dd5f72\n",
      "2025-05-26 22:59:25,594 INFO status has been updated to accepted\n",
      "2025-05-26 22:59:40,095 INFO status has been updated to running\n",
      "2025-05-26 22:59:47,898 INFO status has been updated to successful\n",
      "                                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk2.zip\n",
      "  ✓ Download completed: test2_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - /Users/saket/github/eranest/notebooks/test2_chunk2/data_0.nc\n",
      "  - /Users/saket/github/eranest/notebooks/test2_chunk2/data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 4, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.02 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 7300 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 7300 to 3848 rows\n",
      "✓ Dataset filtering completed in 0.01 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.02 seconds\n",
      "Final DataFrame shape: (3848, 7)\n",
      "Rows in final dataset: 3848\n",
      "  ✓ Filtered data shape: (3848, 7)\n",
      "  ✓ Chunk completed in 25.94 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (23088, 7)\n",
      "\n",
      "--- Temporal Aggregation (monthly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to monthly frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "✓ Aggregation completed in 0.01 seconds\n",
      "✓ Aggregated data shape: (12, 4)\n",
      "✓ Unique lat/long combinations: 962\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test2_output\n",
      "✓ Aggregated data exported to: test2_output/test2_monthly_data.csv\n",
      "✓ Unique lat/longs exported to: test2_output/test2_unique_latlongs.csv\n",
      "✓ Raw data exported to: test2_output/test2_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 74.58 seconds\n",
      "Final dataset shape: (12, 4)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month\n",
      "0  0.001762  267.574188  2024      1\n",
      "1  0.002134  273.594452  2024      2\n",
      "2  0.000817  276.579803  2024      3\n",
      "3  0.002484  280.693359  2024      4\n",
      "4  0.000811  287.699432  2024      5\n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saket/github/eranest/eranest/core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1478: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "/Users/saket/github/eranest/eranest/core.py:1495: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify(\n",
    "    request_id=\"test2\",\n",
    "    variables=[\"2m_temperature\", \"total_precipitation\"],\n",
    "    start_date=dt.datetime(2024, 1, 1),\n",
    "    end_date=dt.datetime(2024, 12, 31),\n",
    "    json_file=\"../data/latvia.geojson\",\n",
    "    frequency=\"monthly\",\n",
    "    resolution=\"0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eranest]",
   "language": "python",
   "name": "conda-env-eranest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
