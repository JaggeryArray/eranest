{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eranest\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eranest.era5ify(request_id, variables, start_date, end_date, json_file, frequency, resolution)\n",
    "# request_id : str, unique identifier for the request\n",
    "# variables : list, list of variables to download\n",
    "# start_date : datetime, start date of the data\n",
    "# end_date : datetime, end date of the data\n",
    "# json_file : str, path to the geojson file containing the area of interest\n",
    "# frequency : str, frequency of the data (hourly, daily, weekly, monthly, yearly), optional (hourly by default)\n",
    "# resolution : float, resolution of the data in degrees (0.1, 0.25, etc.), optional (0.25 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: india.json\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test\n",
      "Variables: ['2m_temperature', 'total_precipitation', 'surface_pressure', '2m_dewpoint_temperature']\n",
      "Date Range: 2023-01-01 to 2023-01-31\n",
      "Frequency: daily\n",
      "Resolution: 0.25°\n",
      "GeoJSON File: india.json\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: india.json\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 35.4940°\n",
      "  South: 7.9655°\n",
      "  East:  97.4026°\n",
      "  West:  68.1766°\n",
      "  Area:  29.2259° × 27.5285°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: False\n",
      "Total days to process: 31\n",
      "Max days per chunk: 14\n",
      "Needs chunking: True\n",
      "Will process 3 chunks C2\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/3\n",
      "==================================================\n",
      "Processing: 2023-01-01 to 2023-01-14\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk1\n",
      "Date range: 2023-01-01 to 2023-01-14\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 17:31:06,959 WARNING [2025-05-26T00:00:00] Service is suffering some disruptions. Please check [here](https://status.ecmwf.int/) for status updates.\n",
      "2025-05-26 17:31:06,960 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 17:31:06,961 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 17:31:08,595 INFO Request ID is 694cfb6d-fea4-4c69-9116-b35f746fb3c6\n",
      "2025-05-26 17:31:08,756 INFO status has been updated to accepted\n",
      "2025-05-26 17:31:17,913 INFO status has been updated to running\n",
      "2025-05-26 17:33:04,738 INFO status has been updated to successful\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk1.zip\n",
      "  ✓ Download completed: test_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk1\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk1\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1192: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.52 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.70 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.23 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 146.30 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/3\n",
      "==================================================\n",
      "Processing: 2023-01-15 to 2023-01-28\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk2\n",
      "Date range: 2023-01-15 to 2023-01-28\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 17:33:38,000 WARNING [2025-05-26T00:00:00] Service is suffering some disruptions. Please check [here](https://status.ecmwf.int/) for status updates.\n",
      "2025-05-26 17:33:38,001 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 17:33:38,001 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 17:33:38,819 INFO Request ID is 347ab348-50a3-49d0-8e03-f205c9a9fa89\n",
      "2025-05-26 17:33:39,001 INFO status has been updated to accepted\n",
      "2025-05-26 17:33:53,462 INFO status has been updated to running\n",
      "2025-05-26 17:35:35,047 INFO status has been updated to successful\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1192: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk2.zip\n",
      "  ✓ Download completed: test_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk2\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk2\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 336, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.49 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 4363632 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 4363632 to 1493856 rows\n",
      "✓ Dataset filtering completed in 2.77 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 3.27 seconds\n",
      "Final DataFrame shape: (1493856, 9)\n",
      "Rows in final dataset: 1493856\n",
      "  ✓ Filtered data shape: (1493856, 9)\n",
      "  ✓ Chunk completed in 145.95 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 3/3\n",
      "==================================================\n",
      "Processing: 2023-01-29 to 2023-01-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test_chunk3\n",
      "Date range: 2023-01-29 to 2023-01-31\n",
      "Area: North: 35.494009507787766, West: 68.1766451353734, South: 7.965534776232332, East: 97.40256147663614\n",
      "Variables: 2m_temperature, total_precipitation, surface_pressure, 2m_dewpoint_temperature\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.25°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 17:36:10,375 WARNING [2025-05-26T00:00:00] Service may suffer disruptions. Please check [here](https://status.ecmwf.int/) for status updates.\n",
      "2025-05-26 17:36:10,376 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 17:36:10,377 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 17:36:11,194 INFO Request ID is 396875b5-b141-4043-8dab-c0ea76cceec9\n",
      "2025-05-26 17:36:11,602 INFO status has been updated to accepted\n",
      "2025-05-26 17:36:21,125 INFO status has been updated to running\n",
      "2025-05-26 17:36:45,805 INFO status has been updated to successful\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1180: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1192: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test_chunk3.zip\n",
      "  ✓ Download completed: test_chunk3.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test_chunk3.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk3\\data_stream-oper_stepType-accum.nc\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test_chunk3\\data_stream-oper_stepType-instant.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_stream-oper_stepType-accum.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "    Processing file 2/2: data_stream-oper_stepType-instant.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 72, 'latitude': 111, 'longitude': 117}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 12987 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.50 seconds\n",
      "  - Points inside: 4446\n",
      "  - Points outside: 8541\n",
      "  - Percentage inside: 34.23%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 935064 rows\n",
      "  ✓ Created lookup set with 4446 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 935064 to 320112 rows\n",
      "✓ Dataset filtering completed in 0.57 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 1.07 seconds\n",
      "Final DataFrame shape: (320112, 9)\n",
      "Rows in final dataset: 320112\n",
      "  ✓ Filtered data shape: (320112, 9)\n",
      "  ✓ Chunk completed in 49.21 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (3307824, 9)\n",
      "\n",
      "--- Temporal Aggregation (daily) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to daily frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m', 'sp', 'd2m']\n",
      "✓ Aggregation completed in 0.62 seconds\n",
      "✓ Aggregated data shape: (31, 8)\n",
      "✓ Unique lat/long combinations: 4446\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test_output\n",
      "✓ Aggregated data exported to: test_output\\test_daily_data.csv\n",
      "✓ Unique lat/longs exported to: test_output\\test_unique_latlongs.csv\n",
      "✓ Raw data exported to: test_output\\test_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 368.64 seconds\n",
      "Final dataset shape: (31, 8)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m            sp         d2m  year  month  day  \\\n",
      "0  0.000155  289.156586  95582.187500  283.245087  2023      1    1   \n",
      "1  0.000184  288.782196  95621.062500  283.129059  2023      1    2   \n",
      "2  0.000141  288.308716  95639.898438  282.865387  2023      1    3   \n",
      "3  0.000200  287.533905  95647.882812  281.997925  2023      1    4   \n",
      "4  0.000143  287.359467  95756.250000  281.820160  2023      1    5   \n",
      "\n",
      "         date  \n",
      "0  2023-01-01  \n",
      "1  2023-01-02  \n",
      "2  2023-01-03  \n",
      "3  2023-01-04  \n",
      "4  2023-01-05  \n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify(request_id = \"test\",\n",
    "                     variables = [\"2m_temperature\", \"total_precipitation\", \"surface_pressure\", \"2m_dewpoint_temperature\"],\n",
    "                     start_date = dt.datetime(2023, 1, 1),\n",
    "                     end_date = dt.datetime(2023, 1, 31),\n",
    "                     json_file = \"india.json\",\n",
    "                     frequency = \"daily\",\n",
    "                     resolution = \"0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded JSON file with utf-8 encoding\n",
      "Valid GeoJSON detected: latvia.geojson\n",
      "\n",
      "============================================================\n",
      "STARTING ERA5 DATA PROCESSING\n",
      "============================================================\n",
      "Request ID: test2\n",
      "Variables: ['2m_temperature', 'total_precipitation']\n",
      "Date Range: 2024-01-01 to 2024-12-31\n",
      "Frequency: monthly\n",
      "Resolution: 0.1°\n",
      "GeoJSON File: latvia.geojson\n",
      "\n",
      "--- Input Validation ---\n",
      "✓ All inputs validated successfully\n",
      "\n",
      "--- Loading GeoJSON File ---\n",
      "Attempting to load: latvia.geojson\n",
      "  Trying encoding 1/4: utf-8\n",
      "✓ Successfully loaded GeoJSON file with utf-8 encoding\n",
      "✓ GeoJSON contains 1 feature(s)\n",
      "\n",
      "--- Calculating Bounding Box ---\n",
      "✓ Bounding Box calculated:\n",
      "  North: 58.0856°\n",
      "  South: 55.6776°\n",
      "  East:  28.2431°\n",
      "  West:  20.9537°\n",
      "  Area:  7.2894° × 2.4080°\n",
      "\n",
      "--- Determining Processing Strategy ---\n",
      "Using monthly dataset: True\n",
      "Total months to process: 12\n",
      "Max months per chunk: 10\n",
      "Needs chunking: True\n",
      "Will process 2 chunks C1\n",
      "\n",
      "==================================================\n",
      "CHUNK 1/2\n",
      "==================================================\n",
      "Processing: 2024-01-01 to 2024-10-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk1\n",
      "Date range: 2024-01-01 to 2024-10-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 17:49:46,105 WARNING [2025-05-26T00:00:00] Service may suffer disruptions. Please check [here](https://status.ecmwf.int/) for status updates.\n",
      "2025-05-26 17:49:46,108 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 17:49:46,110 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 17:49:46,924 INFO Request ID is feec4d19-3891-4f27-a9d9-771286512b75\n",
      "2025-05-26 17:49:47,333 INFO status has been updated to accepted\n",
      "2025-05-26 17:49:56,755 INFO status has been updated to running\n",
      "2025-05-26 17:50:01,978 WARNING Structural differences in grib fields detected when opening in xarray. Opening the grib file safely, however this may result in files with non-intuitive filenames.\n",
      "2025-05-26 17:50:01,979 INFO status has been updated to successful\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1094: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1094: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1107: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk1.zip\n",
      "  ✓ Download completed: test2_chunk1.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk1.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test2_chunk1\\data_0.nc\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test2_chunk1\\data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 10, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 20, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.22 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 36500 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 36500 to 19240 rows\n",
      "✓ Dataset filtering completed in 0.02 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.24 seconds\n",
      "Final DataFrame shape: (19240, 7)\n",
      "Rows in final dataset: 19240\n",
      "  ✓ Filtered data shape: (19240, 7)\n",
      "  ✓ Chunk completed in 22.24 seconds\n",
      "  → Waiting 5 seconds before next chunk...\n",
      "\n",
      "==================================================\n",
      "CHUNK 2/2\n",
      "==================================================\n",
      "Processing: 2024-11-01 to 2024-12-31\n",
      "  → Downloading ERA5 data...\n",
      "Downloading ERA5 data for request: test2_chunk2\n",
      "Date range: 2024-11-01 to 2024-12-31\n",
      "Area: North: 58.085567960911355, West: 20.953674316406246, South: 55.677584411089526, East: 28.243103027343746\n",
      "Variables: 2m_temperature, total_precipitation\n",
      "Variable group: mixed\n",
      "Output format: .zip\n",
      "Grid Resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 17:50:12,011 WARNING [2025-05-26T00:00:00] Service may suffer disruptions. Please check [here](https://status.ecmwf.int/) for status updates.\n",
      "2025-05-26 17:50:12,012 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-26 17:50:12,012 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-26 17:50:12,940 INFO Request ID is 19346ce2-41cf-4960-8b5d-5274d5c82cc8\n",
      "2025-05-26 17:50:13,136 INFO status has been updated to accepted\n",
      "2025-05-26 17:50:35,154 WARNING Structural differences in grib fields detected when opening in xarray. Opening the grib file safely, however this may result in files with non-intuitive filenames.\n",
      "2025-05-26 17:50:35,155 INFO status has been updated to successful\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1094: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1094: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"    ✓ Loaded dataset with shape: {dict(ds.dims)}\")\n",
      "C:\\Users\\Atharva Jagtap\\src\\eranest\\eranest\\core.py:1107: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  ✓ Merged dataset shape: {dict(merged_chunk_ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: test2_chunk2.zip\n",
      "  ✓ Download completed: test2_chunk2.zip\n",
      "  → Extracting files...\n",
      "Extracting zip file: test2_chunk2.zip\n",
      "Extracted NetCDF files:\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test2_chunk2\\data_0.nc\n",
      "  - d:\\Disease Burden Estimation Koita\\eranest demo notebook\\test2_chunk2\\data_1.nc\n",
      "  ✓ Extracted 2 files\n",
      "  → Processing NetCDF files...\n",
      "    Processing file 1/2: data_0.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "    Processing file 2/2: data_1.nc\n",
      "    ✓ Loaded dataset with shape: {'valid_time': 2, 'latitude': 25, 'longitude': 73}\n",
      "  → Merging datasets...\n",
      "  ✓ Merged dataset shape: {'valid_time': 4, 'latitude': 25, 'longitude': 73}\n",
      "  → Filtering by shapefile...\n",
      "Starting optimized filtering process...\n",
      "→ Extracting unique lat/lon coordinates from dataset...\n",
      "✓ Found 1825 unique lat/lon combinations\n",
      "→ Filtering unique coordinates against polygon...\n",
      "✓ Coordinate filtering completed in 0.21 seconds\n",
      "  - Points inside: 962\n",
      "  - Points outside: 863\n",
      "  - Percentage inside: 52.71%\n",
      "→ Filtering original dataset using inside coordinates...\n",
      "  Converting dataset to DataFrame...\n",
      "  ✓ Converted to DataFrame with 7300 rows\n",
      "  ✓ Created lookup set with 962 coordinate pairs\n",
      "  Filtering DataFrame rows...\n",
      "  ✓ Filtered from 7300 to 3848 rows\n",
      "✓ Dataset filtering completed in 0.01 seconds\n",
      "\n",
      "--- Final Filtering Results ---\n",
      "Total processing time: 0.22 seconds\n",
      "Final DataFrame shape: (3848, 7)\n",
      "Rows in final dataset: 3848\n",
      "  ✓ Filtered data shape: (3848, 7)\n",
      "  ✓ Chunk completed in 28.35 seconds\n",
      "\n",
      "--- Combining Chunk Results ---\n",
      "✓ Combined data shape: (23088, 7)\n",
      "\n",
      "--- Temporal Aggregation (monthly) ---\n",
      "→ Performing temporal aggregation...\n",
      "Aggregating data to monthly frequency...\n",
      "Sum columns: ['tp']\n",
      "Max columns: []\n",
      "Min columns: []\n",
      "Rate columns: []\n",
      "Average columns: ['t2m']\n",
      "✓ Aggregation completed in 0.04 seconds\n",
      "✓ Aggregated data shape: (12, 4)\n",
      "✓ Unique lat/long combinations: 962\n",
      "\n",
      "--- Saving Results ---\n",
      "→ Created output directory: test2_output\n",
      "✓ Aggregated data exported to: test2_output\\test2_monthly_data.csv\n",
      "✓ Unique lat/longs exported to: test2_output\\test2_unique_latlongs.csv\n",
      "✓ Raw data exported to: test2_output\\test2_raw_data.csv\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total processing time: 55.74 seconds\n",
      "Final dataset shape: (12, 4)\n",
      "\n",
      "First 5 rows of aggregated data:\n",
      "         tp         t2m  year  month\n",
      "0  0.001762  267.574188  2024      1\n",
      "1  0.002134  273.594452  2024      2\n",
      "2  0.000817  276.579803  2024      3\n",
      "3  0.002484  280.693359  2024      4\n",
      "4  0.000811  287.699432  2024      5\n",
      "\n",
      "============================================================\n",
      "ERA5 DATA PROCESSING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df = eranest.era5ify(request_id = \"test2\",\n",
    "                     variables = [\"2m_temperature\", \"total_precipitation\"],\n",
    "                     start_date = dt.datetime(2024, 1, 1),\n",
    "                     end_date = dt.datetime(2024, 12, 31),\n",
    "                     json_file = \"latvia.geojson\",\n",
    "                     frequency = \"monthly\",\n",
    "                     resolution = \"0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
